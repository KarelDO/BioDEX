{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import Match, Icsr\n",
    "from src.utils import get_matches\n",
    "\n",
    "from datetime import datetime\n",
    "import random\n",
    "import datasets\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration BioDEX--raw_dataset-e1a8735a3d189f31\n",
      "Found cached dataset json (/Users/kldooste/.cache/huggingface/datasets/BioDEX___json/BioDEX--raw_dataset-e1a8735a3d189f31/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dd6fd53ceda4f04b4f02cbd970554d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65648\n"
     ]
    }
   ],
   "source": [
    "# load matches\n",
    "dataset = datasets.load_dataset(\"BioDEX/raw_dataset\")\n",
    "matches = get_matches(dataset['train'])\n",
    "print(len(matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# reactions can not contain commas, else the csv-format fails\n",
    "def normalize_reactions(reaction):\n",
    "    # commas in floats should be points\n",
    "    reaction = re.sub(r'(\\d+),(\\d+)', r'\\1.\\2', reaction)\n",
    "\n",
    "    # commas in enumerations should just be spaces\n",
    "    reaction = re.sub(r', ', r' ', reaction)\n",
    "\n",
    "    # commas in protein structures should be dashes\n",
    "    reaction = re.sub(r'(\\d+\\'),(\\d+\\')', r'\\1-\\2', reaction)\n",
    "\n",
    "    # remove all other commas\n",
    "    reaction = re.sub(r',', r'', reaction)\n",
    "\n",
    "    return reaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments\n",
    "report_cutoff = 10\n",
    "commercial_only = False\n",
    "test_cutoff = datetime(year=2021, month=1, day=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches with <= 10 reports: 62,168\n"
     ]
    }
   ],
   "source": [
    "matches = [m for m in matches if len(m.reports) <= report_cutoff]\n",
    "print(f'Matches with <= {report_cutoff} reports: {len(matches):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches with full text: 18,678\n"
     ]
    }
   ],
   "source": [
    "matches = [m for m in matches if m.article.fulltext]\n",
    "print(f'Matches with full text: {len(matches):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(matches, test_cutoff):\n",
    "    test_matches = []\n",
    "    train_matches = []\n",
    "\n",
    "    for m in matches:\n",
    "        pubdate = datetime.strptime(m.article.pubdate[:4], '%Y')\n",
    "        if  pubdate >= test_cutoff:\n",
    "            test_matches.append(m)\n",
    "        else:\n",
    "            train_matches.append(m)\n",
    "\n",
    "    print(f'Train size: {len(train_matches):,}')\n",
    "    print(f'Test size: {len(test_matches):,}')\n",
    "\n",
    "    return train_matches, test_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 14,429\n",
      "Test size: 4,249\n"
     ]
    }
   ],
   "source": [
    "cutoff = datetime(year=2021, month=1, day=1)\n",
    "train, test = split_data(matches, cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reactions_from_report(report):\n",
    "    # get all reactions and outcomes\n",
    "    reactions = []\n",
    "    for reaction in report.patient.reaction:\n",
    "        if reaction.reactionmeddrapt:\n",
    "            reactions.append(reaction.reactionmeddrapt)\n",
    "    # deduplicate and sort\n",
    "    reactions = sorted(list(set(reactions)))\n",
    "    # normalize\n",
    "    reactions = [normalize_reactions(r) for r in reactions]\n",
    "    return reactions\n",
    "\n",
    "def reactions_from_match(match):\n",
    "    reactions = []\n",
    "    for report in match.reports:\n",
    "        reactions.extend(reactions_from_report(report))\n",
    "    reactions = sorted(list(set(reactions)))\n",
    "    return reactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds_from_split(split):\n",
    "    reactions = []\n",
    "    reactions_unmerged = []\n",
    "    safetyids = []\n",
    "\n",
    "    for match in split:\n",
    "        reactions.append(reactions_from_match(match))\n",
    "        reactions_unmerged.append([reactions_from_report(r) for r in match.reports])\n",
    "        safetyids.append([report.safetyreportid for report in match.reports])\n",
    "\n",
    "    # format the data\n",
    "    data = [m.article.dict() for m in split]\n",
    "    for d, r, r_unm, reportids in zip(data, reactions, reactions_unmerged, safetyids):\n",
    "        d.update({\n",
    "            'reactions': \", \".join(r),\n",
    "            'reactions_unmerged': [\", \".join(r) for r in r_unm],\n",
    "            'safetyreportids': reportids\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data=data)\n",
    "    # reorder some of the columns\n",
    "    df = df[['title', 'abstract','fulltext','reactions', 'reactions_unmerged', 'pmid', 'fulltext_license', 'title_normalized','issue', 'pages', 'journal', 'authors', 'pubdate', 'doi', 'affiliations', 'medline_ta', 'nlm_unique_id', 'issn_linking', 'country', 'mesh_terms', 'publication_types', 'chemical_list', 'keywords', 'references', 'delete', 'pmc', 'other_id', 'safetyreportids']]\n",
    "    df = df.fillna('')\n",
    "    ds = datasets.Dataset.from_pandas(df)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = get_ds_from_split(train)\n",
    "test_ds = get_ds_from_split(test)\n",
    "\n",
    "train_ds = train_ds.shuffle(42)\n",
    "test_ds = test_ds.shuffle(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a validation set\n",
    "len_train = int(len(train_ds) * 0.8)\n",
    "\n",
    "val_ds = train_ds.select(range(len_train,len(train_ds)))\n",
    "train_ds = train_ds.select(range(len_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:  11543\n",
      "val size:  2886\n",
      "test size:  4249\n"
     ]
    }
   ],
   "source": [
    "print('train size: ', len(train_ds))\n",
    "print('val size: ', len(val_ds))\n",
    "print('test size: ', len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess fulltexdt\n",
    "def remove_front(text):\n",
    "    if '==== Body' in text:\n",
    "        text = ('\\n').join(text.split('==== Body')[1:])\n",
    "    return text.strip()\n",
    "\n",
    "def remove_refs(text):\n",
    "    if '==== Refs' in text:\n",
    "        text = ('\\n').join(text.split('==== Refs')[:-1])\n",
    "    return text.strip() \n",
    "\n",
    "def get_fulltext_input(row, fulltext_only=False):\n",
    "    fulltext_filtered = remove_refs(remove_front(row['fulltext']))\n",
    "    data = ['\\nTITLE:', row['title'], '\\nABSTRACT:', row['abstract']]\n",
    "    if fulltext_only:\n",
    "        data.extend(['\\nTEXT:', fulltext_filtered])\n",
    "    return ('\\n').join(data).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.DatasetDict({\n",
    "    'train': train_ds,\n",
    "    'validation': val_ds,\n",
    "    'test': test_ds\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68c05987a99f4cb2a408ebd12ecd0c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11543 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e50e5e0a555485f9942d29789b4e536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2886 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a610bbfd844799bbb5c94d8ef1db2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4249 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = ds.map(lambda example: {'fulltext_processed': get_fulltext_input(example, fulltext_only=True)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split train to the Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35660d5828fb424c8185c3e3a49beac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bfc101737ca4421876fd790b2e5e57f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f882cb54b74a71bd425232eef72705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "323b75a432bb479bb22e65091c176527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Deleting unused files from dataset repository:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split validation to the Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5fe5c5cb0d5487ebf60933002fd151a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9edce6945224423d8530e89bd0514462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab06a6106044fb6a2e654735f078a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split test to the Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "255367c012954cd3b29f778e14d5e08b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b426005fce904849a5b368f88b906588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f7fa2fbb4bb4ea3b6c42fda23e710da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6232040bda344a1a32182642e9ddbaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.59k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# upload data\n",
    "ds.push_to_hub('BioDEX/BioDEX-Reactions')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biodex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
