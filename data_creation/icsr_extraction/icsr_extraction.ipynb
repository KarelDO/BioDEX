{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import Match, Icsr\n",
    "from src.utils import get_matches\n",
    "\n",
    "from datetime import datetime\n",
    "import random\n",
    "import datasets\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration BioDEX--raw_dataset-e1a8735a3d189f31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/BioDEX--raw_dataset to /Users/kldooste/.cache/huggingface/datasets/BioDEX___json/BioDEX--raw_dataset-e1a8735a3d189f31/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2906ffa4374a427ab0ff51e3c1435ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f83dd3936763417f8d61a742cab93e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc66d7a80ae4e22bb809bded454c2e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /Users/kldooste/.cache/huggingface/datasets/BioDEX___json/BioDEX--raw_dataset-e1a8735a3d189f31/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce0aa15c74c84dfbae0876682fcffe58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65648\n"
     ]
    }
   ],
   "source": [
    "# load matches\n",
    "dataset = datasets.load_dataset(\"BioDEX/raw_dataset\")\n",
    "matches = get_matches(dataset['train'])\n",
    "print(len(matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments\n",
    "report_cutoff = 10\n",
    "fulltext_only = True\n",
    "# fulltext_only = False\n",
    "commercial_only = False\n",
    "test_cutoff = datetime(year=2021, month=1, day=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter too many reports\n",
    "A few articles have many reports. They are typically survey articles, drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches with <= 10 reports: 62,168\n"
     ]
    }
   ],
   "source": [
    "matches = [m for m in matches if len(m.reports) <= report_cutoff]\n",
    "print(f'Matches with <= {report_cutoff} reports: {len(matches):,}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get articles with a full text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches with full text: 18,678\n"
     ]
    }
   ],
   "source": [
    "if fulltext_only:\n",
    "    matches = [m for m in matches if m.article.fulltext]\n",
    "    print(f'Matches with full text: {len(matches):,}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (optional) Get articles with commercial license"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noncommercial_licenses = {'CC BY-NC', 'CC BY-NC-SA', 'CC BY-NC-ND'}\n",
    "commercial_licenses = {'CC0', 'CC BY', 'CC BY-SA', 'CC BY-ND'}\n",
    "\n",
    "if commercial_only:\n",
    "    matches = [m for m in matches if m.article.fulltext_license in commercial_licenses]\n",
    "    print(f'Fulltext commercial dataset: {len(matches):,}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(matches, test_cutoff):\n",
    "    test_matches = []\n",
    "    train_matches = []\n",
    "\n",
    "    for m in matches:\n",
    "        pubdate = datetime.strptime(m.article.pubdate[:4], '%Y')\n",
    "        if  pubdate >= test_cutoff:\n",
    "            test_matches.append(m)\n",
    "        else:\n",
    "            train_matches.append(m)\n",
    "\n",
    "    print(f'Train size: {len(train_matches):,}')\n",
    "    print(f'Test size: {len(test_matches):,}')\n",
    "\n",
    "    return train_matches, test_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 14,429\n",
      "Test size: 4,249\n"
     ]
    }
   ],
   "source": [
    "cutoff = datetime(year=2021, month=1, day=1)\n",
    "train, test = split_data(matches, cutoff)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format the data\n",
    "For every article, sample one report as target.\n",
    "\n",
    "Drop some fields and upload to huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_icsrs_from_split(split):\n",
    "    random.seed(42)\n",
    "\n",
    "    icsrs = []\n",
    "    reportids = []\n",
    "    for m in split:\n",
    "        # get all valid icsrs \n",
    "        new_icsrs = [(Icsr.from_report(r), r.safetyreportid) for r in m.reports]\n",
    "        new_icsrs = [t for t in new_icsrs if t[0]]\n",
    "        # sample one\n",
    "        if new_icsrs:\n",
    "            new_icsr, new_reportid = random.choice(new_icsrs)\n",
    "            icsrs.append(new_icsr)\n",
    "            reportids.append(new_reportid)\n",
    "        else:\n",
    "            icsrs.append(None)\n",
    "            reportids.append(None)\n",
    "    return icsrs, reportids\n",
    "\n",
    "def get_ds_from_split(split):\n",
    "    icsrs, reportids = get_icsrs_from_split(split)\n",
    "    # only keep data with valid icsrs\n",
    "    valid = [m for index,m in enumerate(split) if icsrs[index]]\n",
    "    reportids = [reportids[i] for i in range(len(reportids)) if icsrs[i]]\n",
    "    icsrs = [i for i in icsrs if i]\n",
    "    print(f'Found samples with valid icsrs: {len(valid):,}')\n",
    "\n",
    "    # format the data\n",
    "    data = [m.article.dict() for m in valid]\n",
    "    for d, i, reportid in zip(data, icsrs, reportids):\n",
    "        d.update({\n",
    "            'target': i.to_string(),\n",
    "            'safetyreportid': reportid\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data=data)\n",
    "    # reorder some of the columns\n",
    "    df = df[['title', 'abstract','fulltext','target', 'pmid', 'fulltext_license', 'title_normalized','issue', 'pages', 'journal', 'authors', 'pubdate', 'doi', 'affiliations', 'medline_ta', 'nlm_unique_id', 'issn_linking', 'country', 'mesh_terms', 'publication_types', 'chemical_list', 'keywords', 'references', 'delete', 'pmc', 'other_id', 'safetyreportid']]\n",
    "    df = df.fillna('')\n",
    "    ds = datasets.Dataset.from_pandas(df)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found samples with valid icsrs: 12,031\n",
      "Found samples with valid icsrs: 3,628\n"
     ]
    }
   ],
   "source": [
    "train_ds = get_ds_from_split(train)\n",
    "test_ds = get_ds_from_split(test)\n",
    "\n",
    "train_ds = train_ds.shuffle(42)\n",
    "test_ds = test_ds.shuffle(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'abstract', 'fulltext', 'target', 'pmid', 'fulltext_license', 'title_normalized', 'issue', 'pages', 'journal', 'authors', 'pubdate', 'doi', 'affiliations', 'medline_ta', 'nlm_unique_id', 'issn_linking', 'country', 'mesh_terms', 'publication_types', 'chemical_list', 'keywords', 'references', 'delete', 'pmc', 'other_id', 'safetyreportid'],\n",
       "    num_rows: 12031\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a validation set\n",
    "len_train = int(len(train_ds) * 0.8)\n",
    "\n",
    "val_ds = train_ds.select(range(len_train,len(train_ds)))\n",
    "train_ds = train_ds.select(range(len_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:  9624\n",
      "val size:  2407\n",
      "test size:  3628\n"
     ]
    }
   ],
   "source": [
    "print('train size: ', len(train_ds))\n",
    "print('val size: ', len(val_ds))\n",
    "print('test size: ', len(test_ds))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess fulltext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_front(text):\n",
    "    if '==== Body' in text:\n",
    "        text = ('\\n').join(text.split('==== Body')[1:])\n",
    "    return text.strip()\n",
    "\n",
    "def remove_refs(text):\n",
    "    if '==== Refs' in text:\n",
    "        text = ('\\n').join(text.split('==== Refs')[:-1])\n",
    "    return text.strip() \n",
    "\n",
    "def get_fulltext_input(row, fulltext_only=False):\n",
    "    fulltext_filtered = remove_refs(remove_front(row['fulltext']))\n",
    "    data = ['\\nTITLE:', row['title'], '\\nABSTRACT:', row['abstract']]\n",
    "    if fulltext_only:\n",
    "        data.extend(['\\nTEXT:', fulltext_filtered])\n",
    "    return ('\\n').join(data).strip()\n",
    "\n",
    "# numbered_titles_re = r'^(\\d[\\.\\d]*) (.*)\\n'\n",
    "# capitalized_titles_re = r'^([A-Z ]+)\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.DatasetDict({\n",
    "    'train': train_ds,\n",
    "    'validation': val_ds,\n",
    "    'test': test_ds\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e39a2f3d0ea4b088093d1707ddef5ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9624 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "867736d20b914879a5dea83da7c40188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2407 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc639a49b26483e86574ac39c777efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3628 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = ds.map(lambda example: {'fulltext_processed': get_fulltext_input(example, fulltext_only=fulltext_only)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'abstract', 'fulltext', 'target', 'pmid', 'fulltext_license', 'title_normalized', 'issue', 'pages', 'journal', 'authors', 'pubdate', 'doi', 'affiliations', 'medline_ta', 'nlm_unique_id', 'issn_linking', 'country', 'mesh_terms', 'publication_types', 'chemical_list', 'keywords', 'references', 'delete', 'pmc', 'other_id', 'safetyreportid', 'fulltext_processed'],\n",
       "        num_rows: 9624\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['title', 'abstract', 'fulltext', 'target', 'pmid', 'fulltext_license', 'title_normalized', 'issue', 'pages', 'journal', 'authors', 'pubdate', 'doi', 'affiliations', 'medline_ta', 'nlm_unique_id', 'issn_linking', 'country', 'mesh_terms', 'publication_types', 'chemical_list', 'keywords', 'references', 'delete', 'pmc', 'other_id', 'safetyreportid', 'fulltext_processed'],\n",
       "        num_rows: 2407\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'abstract', 'fulltext', 'target', 'pmid', 'fulltext_license', 'title_normalized', 'issue', 'pages', 'journal', 'authors', 'pubdate', 'doi', 'affiliations', 'medline_ta', 'nlm_unique_id', 'issn_linking', 'country', 'mesh_terms', 'publication_types', 'chemical_list', 'keywords', 'references', 'delete', 'pmc', 'other_id', 'safetyreportid', 'fulltext_processed'],\n",
       "        num_rows: 3628\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split train to the Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877ce661f9424ce8b10f3ad1a9726b40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea27e53ee1e74714b336de14eca9894b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ee390b7f07447db917b61da53bde9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split validation to the Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "399bbc385efd4bf5871a3d7bbbe753ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "247d7d7b1c69488694150bb34a860aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f92d284fc3bc4466923e645d7301cc50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split test to the Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccfc75bb56484310b75fa45bbfc1f303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a10af84b8c14f2884029bf133a8f31e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e4943067b94c0f9d618c9537e7761e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if fulltext_only:\n",
    "    ds.push_to_hub('BioDEX/BioDEX-ICSR')\n",
    "else:\n",
    "    ds.push_to_hub('BioDEX/BioDEX-ICSR-Abstract')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration BioDEX--BioDEX-ICSR-56e19d4a04830a55\n",
      "Found cached dataset parquet (/Users/kldooste/.cache/huggingface/datasets/BioDEX___parquet/BioDEX--BioDEX-ICSR-56e19d4a04830a55/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc465b4f7ed4d36a4b1b9b203083bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = datasets.load_dataset('BioDEX/BioDEX-ICSR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1990, 2020)\n",
      "(1985, 2020)\n",
      "(2021, 2022)\n"
     ]
    }
   ],
   "source": [
    "# get the ranges\n",
    "def get_ranges(dates):\n",
    "    dates = [int(d[:4]) for d  in dates]\n",
    "    return min(dates), max(dates)\n",
    "\n",
    "print(get_ranges(ds['train']['pubdate']))\n",
    "print(get_ranges(ds['validation']['pubdate']))\n",
    "print(get_ranges(ds['test']['pubdate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163.05382377389859\n",
      "1325\n",
      "The percentile of 128 is 69.0\n"
     ]
    }
   ],
   "source": [
    "# average target length\n",
    "target_lengths = [len(e['target']) for e in ds['train']]\n",
    "print(sum(target_lengths) / len(target_lengths))\n",
    "print(max(target_lengths))\n",
    "\n",
    "# get percentile of a length\n",
    "# Calculate percentile\n",
    "import numpy as np\n",
    "length = 128\n",
    "percentile = np.percentile(target_lengths, (length / len(target_lengths)) * 100)\n",
    "\n",
    "# Print percentile\n",
    "print(\"The percentile of\", length, \"is\", percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
