nlprun -g 1 -m jagupard32 -r 32G -n t5-large-lowerlr-longercontext 'python run_encdec_for_icsr_extraction.py --overwrite_cache False --seed 42 --dataset_name FAERS-PubMed/BioDEX-ICSR --text_column fulltext_processed --summary_column target --model_name_or_path google/flan-t5-large --output_dir ../../checkpoints/flan-t5-large-BioDEX-ICSR-lowerlr-longercontext --max_source_length 1024 --max_target_length 128 --do_train True --do_eval True --lr_scheduler_type linear --warmup_ratio 0.0 --learning_rate 0.0001 --optim adafactor --per_device_train_batch_size 4 --per_device_eval_batch_size 16 --gradient_accumulation_steps 1 --eval_accumulation_steps 16 --num_train_epochs 10 --bf16 True --evaluation_strategy epoch --logging_strategy steps --save_strategy epoch --logging_steps 100 --save_total_limit 1 --report_to wandb --load_best_model_at_end True --metric_for_best_model loss --greater_is_better False'

nlprun -g 1 -m jagupard32 -r 32G -n t5-large-lowerlr 'python run_encdec_for_icsr_extraction.py --overwrite_cache False --seed 42 --dataset_name FAERS-PubMed/BioDEX-ICSR --text_column fulltext_processed --summary_column target --model_name_or_path google/flan-t5-large --output_dir ../../checkpoints/flan-t5-large-BioDEX-ICSR-lowerlr --max_source_length 512 --max_target_length 128 --do_train True --do_eval True --lr_scheduler_type linear --warmup_ratio 0.0 --learning_rate 0.0001 --optim adafactor --per_device_train_batch_size 8 --per_device_eval_batch_size 16 --gradient_accumulation_steps 1 --eval_accumulation_steps 16 --num_train_epochs 10 --bf16 True --evaluation_strategy epoch --logging_strategy steps --save_strategy epoch --logging_steps 100 --save_total_limit 1 --report_to wandb --load_best_model_at_end True --metric_for_best_model loss --greater_is_better False'

nlprun -g 1 -m jagupard32 -r 32G -n t5-large-warmup 'python run_encdec_for_icsr_extraction.py --overwrite_cache False --seed 42 --dataset_name FAERS-PubMed/BioDEX-ICSR --text_column fulltext_processed --summary_column target --model_name_or_path google/flan-t5-large --output_dir ../../checkpoints/flan-t5-large-BioDEX-ICSR-warmup --max_source_length 512 --max_target_length 128 --do_train True --do_eval True --lr_scheduler_type linear --warmup_ratio 0.2 --learning_rate 0.001 --optim adafactor --per_device_train_batch_size 8 --per_device_eval_batch_size 16 --gradient_accumulation_steps 1 --eval_accumulation_steps 16 --num_train_epochs 10 --bf16 True --evaluation_strategy epoch --logging_strategy steps --save_strategy epoch --logging_steps 100 --save_total_limit 1 --report_to wandb --load_best_model_at_end True --metric_for_best_model loss --greater_is_better False'

nlprun -g 1 -m jagupard33 -r 32G -n t5-large 'python run_encdec_for_icsr_extraction.py --overwrite_cache False --seed 42 --dataset_name FAERS-PubMed/BioDEX-ICSR --text_column fulltext_processed --summary_column target --model_name_or_path google/flan-t5-large --output_dir ../../checkpoints/flan-t5-large-BioDEX-ICSR --max_source_length 512 --max_target_length 128 --do_train True --do_eval True --lr_scheduler_type linear --warmup_ratio 0.0 --learning_rate 0.001 --optim adafactor --per_device_train_batch_size 8 --per_device_eval_batch_size 16 --gradient_accumulation_steps 1 --eval_accumulation_steps 16 --num_train_epochs 10 --bf16 True --evaluation_strategy epoch --logging_strategy steps --save_strategy epoch --logging_steps 100 --save_total_limit 1 --report_to wandb --load_best_model_at_end True --metric_for_best_model loss --greater_is_better False'

nlprun -g 1 -m jagupard33 -r 32G -n t5-large-lowerlr-longercontext2 'python run_encdec_for_icsr_extraction.py --overwrite_cache False --seed 42 --dataset_name FAERS-PubMed/BioDEX-ICSR --text_column fulltext_processed --summary_column target --model_name_or_path google/flan-t5-large --output_dir ../../checkpoints/flan-t5-large-BioDEX-ICSR-lowerlr-longercontext2 --max_source_length 2048 --max_target_length 128 --do_train True --do_eval True --lr_scheduler_type linear --warmup_ratio 0.0 --learning_rate 0.0001 --optim adafactor --per_device_train_batch_size 1 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --eval_accumulation_steps 16 --num_train_epochs 10 --bf16 True --evaluation_strategy epoch --logging_strategy steps --save_strategy epoch --logging_steps 100 --save_total_limit 1 --report_to wandb --load_best_model_at_end True --metric_for_best_model loss --greater_is_better False'


nlprun -g 1 -m jagupard33 -r 32G -n t5-large-lowerlr-longercontext-longertarget 'python run_encdec_for_icsr_extraction.py --overwrite_cache False --seed 42 --dataset_name FAERS-PubMed/BioDEX-ICSR --text_column fulltext_processed --summary_column target --model_name_or_path google/flan-t5-large --output_dir ../../checkpoints/flan-t5-large-BioDEX-ICSR-lowerlr-longercontext-longertarget --max_source_length 1024 --max_target_length 256 --do_train True --do_eval True --lr_scheduler_type linear --warmup_ratio 0.0 --learning_rate 0.0001 --optim adafactor --per_device_train_batch_size 4 --per_device_eval_batch_size 16 --gradient_accumulation_steps 1 --eval_accumulation_steps 16 --num_train_epochs 10 --bf16 True --evaluation_strategy epoch --logging_strategy steps --save_strategy epoch --logging_steps 100 --save_total_limit 1 --report_to wandb --load_best_model_at_end True --metric_for_best_model loss --greater_is_better False'

nlprun -g 1 -m jagupard33 -r 32G -n t5-xl 'python run_encdec_for_icsr_extraction.py --overwrite_cache False --seed 42 --dataset_name FAERS-PubMed/BioDEX-ICSR --text_column fulltext_processed --summary_column target --model_name_or_path google/flan-t5-xl --output_dir ../../checkpoints/flan-t5-xl-BioDEX-ICSR --max_source_length 512 --max_target_length 256 --do_train True --do_eval True --lr_scheduler_type linear --warmup_ratio 0.0 --learning_rate 0.0001 --optim adafactor --per_device_train_batch_size 2 --per_device_eval_batch_size 8 --gradient_accumulation_steps 2 --eval_accumulation_steps 16 --num_train_epochs 10 --bf16 True --evaluation_strategy epoch --logging_strategy steps --save_strategy epoch --logging_steps 100 --save_total_limit 1 --report_to wandb --load_best_model_at_end True --metric_for_best_model loss --greater_is_better False'

nlprun -g 1 -m jagupard33 -r 32G -n t5-xl-longercontext 'python run_encdec_for_icsr_extraction.py --overwrite_cache False --seed 42 --dataset_name FAERS-PubMed/BioDEX-ICSR --text_column fulltext_processed --summary_column target --model_name_or_path google/flan-t5-xl --output_dir ../../checkpoints/flan-t5-xl-BioDEX-ICSR-longercontext --max_source_length 1024 --max_target_length 256 --do_train True --do_eval True --lr_scheduler_type linear --warmup_ratio 0.0 --learning_rate 0.0001 --optim adafactor --per_device_train_batch_size 1 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --eval_accumulation_steps 16 --num_train_epochs 10 --bf16 True --evaluation_strategy epoch --logging_strategy steps --save_strategy epoch --logging_steps 100 --save_total_limit 1 --report_to wandb --load_best_model_at_end True --metric_for_best_model loss --greater_is_better False'


nlprun -g 1 -m jagupard33 -r 32G -n t5-large-lowerlr-longercontext2-prompt 'python run_encdec_for_icsr_extraction.py --overwrite_cache False --seed 42 --dataset_name FAERS-PubMed/BioDEX-ICSR --text_column fulltext_processed --summary_column target --model_name_or_path google/flan-t5-large --output_dir ../../checkpoints/flan-t5-large-BioDEX-ICSR-lowerlr-longercontext2-prompt --max_source_length 2048 --max_target_length 128 --do_train True --do_eval True --lr_scheduler_type linear --warmup_ratio 0.0 --learning_rate 0.0001 --optim adafactor --per_device_train_batch_size 1 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --eval_accumulation_steps 16 --num_train_epochs 5 --bf16 True --evaluation_strategy epoch --logging_strategy steps --save_strategy epoch --logging_steps 100 --save_total_limit 1 --report_to wandb --load_best_model_at_end True --metric_for_best_model loss --greater_is_better False --source_prefix "summarize adverse drug events:"'


nlprun -g 1 -d a6000 -r 32G -n t5-large-lowerlr-longercontext2-lora 'python run_encdec_for_icsr_extraction.py --overwrite_cache False --seed 42 --dataset_name FAERS-PubMed/BioDEX-ICSR --text_column fulltext_processed --summary_column target --model_name_or_path google/flan-t5-large --output_dir ../../checkpoints/flan-t5-large-BioDEX-ICSR-lowerlr-longercontext2-lora --max_source_length 2048 --max_target_length 128 --do_train True --do_eval True --lr_scheduler_type linear --warmup_ratio 0.0 --learning_rate 0.0001 --optim adafactor --per_device_train_batch_size 2 --per_device_eval_batch_size 8 --gradient_accumulation_steps 2 --eval_accumulation_steps 16 --num_train_epochs 10 --bf16 True --evaluation_strategy epoch --logging_strategy steps --save_strategy epoch --logging_steps 100 --save_total_limit 1 --report_to wandb --load_best_model_at_end True --metric_for_best_model loss --greater_is_better False --use_peft_lora True'

nlprun -g 1 -d a6000 -r 32G -n t5-large-lowerlr-longercontext2-longertarget 'python run_encdec_for_icsr_extraction.py --overwrite_cache False --seed 42 --dataset_name FAERS-PubMed/BioDEX-ICSR --text_column fulltext_processed --summary_column target --model_name_or_path google/flan-t5-large --output_dir ../../checkpoints/flan-t5-large-BioDEX-ICSR-lowerlr-longercontext2-longertarget --max_source_length 2048 --max_target_length 256 --do_train True --do_eval True --lr_scheduler_type linear --warmup_ratio 0.0 --learning_rate 0.0001 --optim adafactor --per_device_train_batch_size 1 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --eval_accumulation_steps 16 --num_train_epochs 5 --bf16 True --evaluation_strategy epoch --logging_strategy steps --save_strategy epoch --logging_steps 100 --save_total_limit 1 --report_to wandb --load_best_model_at_end True --metric_for_best_model loss --greater_is_better False'

nlprun -g 1 -m sphinx1 -r 32G -n t5-base-s4096-t256 'python run_encdec_for_icsr_extraction.py --overwrite_cache False --seed 42 --dataset_name FAERS-PubMed/BioDEX-ICSR --text_column fulltext_processed --summary_column target --model_name_or_path google/flan-t5-base --output_dir ../../checkpoints/flan-t5-base-BioDEX-ICSR-s4096-t256 --max_source_length 4096 --max_target_length 256 --do_train True --do_eval True --lr_scheduler_type linear --warmup_ratio 0.0 --learning_rate 0.0001 --optim adafactor --per_device_train_batch_size 1 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --eval_accumulation_steps 16 --num_train_epochs 10 --bf16 True --evaluation_strategy epoch --logging_strategy steps --save_strategy epoch --logging_steps 100 --save_total_limit 1 --report_to wandb --load_best_model_at_end True --metric_for_best_model loss --greater_is_better False --use_peft_lora True'

# eval a run
nlprun -g 1 -d a5000 -r 32G -n eval-t5-large-lowerlr-longercontext2-eval 'python run_encdec_for_icsr_extraction.py --overwrite_cache False --seed 42 --dataset_name FAERS-PubMed/BioDEX-ICSR --text_column fulltext_processed --summary_column target --model_name_or_path ../../checkpoints/flan-t5-large-BioDEX-ICSR-lowerlr-longercontext2 --output_dir ../../checkpoints/flan-t5-large-BioDEX-ICSR-lowerlr-longercontext2 --max_source_length 2048 --max_target_length 128 --do_train False --do_eval True --lr_scheduler_type linear --warmup_ratio 0.0 --learning_rate 0.0001 --optim adafactor --per_device_train_batch_size 1 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --eval_accumulation_steps 16 --num_train_epochs 10 --bf16 True --evaluation_strategy epoch --logging_strategy steps --save_strategy epoch --logging_steps 100 --save_total_limit 1 --report_to wandb --load_best_model_at_end True --metric_for_best_model loss --greater_is_better False --predict_with_generate True --generation_max_length 128 --num_beams 1 --repetition_penalty 1.0'

nlprun -g 1 -d a5000 -r 32G -n eval-t5-large-lowerlr-longercontext2-eval2 'python run_encdec_for_icsr_extraction.py --overwrite_cache False --seed 42 --dataset_name FAERS-PubMed/BioDEX-ICSR --text_column fulltext_processed --summary_column target --model_name_or_path ../../checkpoints/flan-t5-large-BioDEX-ICSR-lowerlr-longercontext2 --output_dir ../../checkpoints/flan-t5-large-BioDEX-ICSR-lowerlr-longercontext2 --max_source_length 2048 --max_target_length 128 --do_train False --do_eval True --lr_scheduler_type linear --warmup_ratio 0.0 --learning_rate 0.0001 --optim adafactor --per_device_train_batch_size 1 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --eval_accumulation_steps 16 --num_train_epochs 10 --bf16 True --evaluation_strategy epoch --logging_strategy steps --save_strategy epoch --logging_steps 100 --save_total_limit 1 --report_to wandb --load_best_model_at_end True --metric_for_best_model loss --greater_is_better False --predict_with_generate True --generation_max_length 128 --num_beams 1 --repetition_penalty 1.2'

nlprun -g 1 -d a5000 -r 32G -n t5-large-lowerlr-longercontext2-eval3 'python run_encdec_for_icsr_extraction.py --overwrite_cache False --seed 42 --dataset_name FAERS-PubMed/BioDEX-ICSR --text_column fulltext_processed --summary_column target --model_name_or_path ../../checkpoints/flan-t5-large-BioDEX-ICSR-lowerlr-longercontext2 --output_dir ../../checkpoints/flan-t5-large-BioDEX-ICSR-lowerlr-longercontext2 --max_source_length 2048 --max_target_length 128 --do_train False --do_eval True --lr_scheduler_type linear --warmup_ratio 0.0 --learning_rate 0.0001 --optim adafactor --per_device_train_batch_size 1 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --eval_accumulation_steps 16 --num_train_epochs 10 --bf16 True --evaluation_strategy epoch --logging_strategy steps --save_strategy epoch --logging_steps 100 --save_total_limit 1 --report_to wandb --load_best_model_at_end True --metric_for_best_model loss --greater_is_better False --predict_with_generate True --generation_max_length 128 --num_beams 2 --repetition_penalty 1.2'